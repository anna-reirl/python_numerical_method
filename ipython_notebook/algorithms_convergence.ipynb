{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Algorithms and Convergence\n",
    "- Title: Methods of Numerical Analysis (Python)\n",
    "- Date: Date: Aug/26/2015, Wednesday - Dec/21/2015, Monday\n",
    "- Author: Minwoo Bae (minubae.nyc@gmail.com)\n",
    "- Reference: Numerical Analysis - 10th Edition by Richard L. Burden, Douglas J. Faires, and Annette M. Burden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color:#FFFF00\">Approximation procedures, called $\\mathit{algorithms}$, involve sequences of calculations.</span> An $\\mathbf{algorithm}$ is a procedure that describes, in an unambiguous manner, a finite sequence of steps to be performed in a specified order. The object of the algorithm is to implement a procedure to solve a problem or approximate a solution to the problem.\n",
    "\n",
    "### Illustration\n",
    "The following algorithm computes $x_{1} + x_{2} + \\dots + x_{N} = \\displaystyle\\sum_{i=1}^{N} x_i$, given $N$ and the numbers $x_{1},x_{2},\\dots,x_{N}$.\n",
    "<br>\n",
    "$\\mathsf{INPUT} \\space\\space\\space N, x_{1}, x_{2},\\dots, x_{N}.$\n",
    "<br><br>\n",
    "$\\mathsf{OUTPUT} \\space\\space\\space SUM = \\sum_{i=1}^{N} x_i.$\n",
    "<br><br>\n",
    "$\\mathsf{Step\\space 1} \\space\\space\\space Set \\space SUM = 0. \\space\\space (Initialize accumulator.)$\n",
    "<br><br>\n",
    "$\\mathsf{Step\\space 2} \\space\\space\\space For \\space i = 1,2,\\dots,N \\space do \\space\\space (Add the next term.)$\n",
    "<br><br>\n",
    "$\\mathsf{Step\\space 3} \\space\\space\\space OUTPUT \\space (SUM);$\n",
    "<br>\n",
    "$\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space STOP.$\n",
    "\n",
    "## 2.1 Characterizing Algorithms\n",
    "We need to determine approximation methods that produce dependably accurate results for a wide class of problem. Because of the differing ways in which the approximation methods are derived, we need to a variety of conditions to categorize their accuracy. Not all of these conditions will be appropriate for any particular problem.\n",
    "<br><br>\n",
    "<span style=\"background-color:#FFFF00\">One criterion we will impose on an algorithm whenever possible is that small changes in the initial data produce correspondingly small changes in the final results.</span> An algorithm that satisfies this property is called $\\mathbf{stable}$; otherwise, it is $\\mathbf{unstable}$. Some algorithms are stable only for certain choices of initial data and are called $\\mathbf{conditionally \\space stable}$. We will characterize the stability properties of algorithms whenever possible.\n",
    "<br><br>\n",
    "To further consider the subject of round-off error growth and its connection to algorithm stability, suppose an error with magnitude $E_{0} > 0$ is introduced at some stage in the calculations and that the magnitude of the error after $n$ subsequent operations is denoted by $E_{n}$. The two cases that arise most often in practice are defined as follows.\n",
    "\n",
    "### Definition 1.17\n",
    "Suppose that $E_{0} > 0$ denotes an error introduced at some stage in the calculations and $E_{n}$ represents the magnitude of the error after $n$ subsequent operations.\n",
    "\n",
    "- If $E_{n} \\approx CnE_{0}$, where $C$ is a constant independent of $n$, then the growth of error is said to be $\\mathbf{linear}$.\n",
    "- If $E_{n} \\approx C^{n}E_{0}$, for some $C > 1$, the growth of error is called $\\mathbf{exponential}$.\n",
    "\n",
    "Linear growth of error is usually unavoidable, and when $C$ and $E_{0}$ are small, the results are generally acceptable. Exponential growth of error should be avoided because the term $C^{n}$ becomes large for even relatively small values of $n$. This leads to unacceptable inaccuracies, regardless of the size of $E_{0}$. As a consequence, an algorithm that exhibits linear growth of error is stable, whereas an algorithm exhibiting exponential error growth is unstable.\n",
    "<img src=\"img/error_growth.jpg\" alt=\"Error Growth\" style=\"width:600px; height:400px;\">\n",
    "\n",
    "## 2.2 Rates of Convergence\n",
    "Since iterative techniques involving sesquences are often used, this section concludes with a brief discussion of some terminology used to describe the rate at which convergence occurs. In general, we would like the technique to converge as rapidly as possible. The following definition is used to compare the convergence rates of sequences.\n",
    "\n",
    "### Definition 1.18\n",
    "Suppose $\\{\\beta_{n}\\}_{n=1}^\\infty$ is a sequence known to converge to zero and $\\{\\alpha_{n}\\}_{n=1}^\\infty$ converges to a number $\\alpha$. If a positive constant $K$ exists with\n",
    "<br><br>\n",
    "$$\\big|\\alpha_{n}-\\alpha\\big| \\leq K|\\beta_{n}|, \\space\\space\\space for \\space large \\space n,$$\n",
    "<br>\n",
    "then we say that $\\{\\alpha_{n}\\}_{n=1}^\\infty$ converges to $\\alpha$ with $\\mathbf{rate}, \\space \\mathbf{or\\space order}, \\mathbf{of\\space convergence} \\space\\space O(\\beta_{n})$. (This expression is read \"big oh of $\\beta_{n}$\".) It is indicated by writing $\\alpha_{n} = \\alpha + O(\\beta_{n})$.\n",
    "<br><br>\n",
    "Although Definition 1.18 permits $\\{\\alpha_{n}\\}_{n=1}^\\infty$ to be compared with an arbitrary sequence $\\{\\beta_{n}\\}_{n=1}^\\infty$, in nearly every situation we use\n",
    "<br><br>\n",
    "$$\\beta_{n} = \\frac{1}{n^P},$$\n",
    "<br>\n",
    "for some number $p > 0$. We are generally interested in the largest value of $p$ with $\\alpha_{n} = \\alpha + O(\\frac{1}{n^P})$.\n",
    "\n",
    "### Example 1\n",
    "Suppose that, for $n \\geq 1$,\n",
    "<br><br>\n",
    "$$\\alpha_{n} = \\frac{n+1}{n^2} \\space\\space and \\space\\space \\hat{\\alpha_{n}} = \\frac{n+3}{n^3}.$$\n",
    "<br>\n",
    "Both $\\lim_{n \\to \\infty}\\alpha_{n} = 0$ and $\\lim_{n \\to \\infty}\\hat{\\alpha_{n}} = 0$, but the sequence $\\{\\hat{\\alpha_{n}}\\}$ converges to this limit much faster than the sequence $\\{\\alpha_{n}\\}$. Using five-digit rounding arithmetic, we have the values shown in the following Table. Determine rates of convergence for these two sequences.\n",
    "#### Solution\n",
    "Define the sequences $\\beta_{n} = 1/n$ and $\\hat{\\beta_{n}} = 1/n^2$. Then\n",
    "<br><br>\n",
    "$$\\big|\\alpha_{n} - 0\\big| = \\frac{n+1}{n^2} \\leq \\frac{n+n}{n^2} = 2 \\cdot \\frac{1}{n} = 2\\beta_{n}$$\n",
    "<br>\n",
    "and\n",
    "$$\\big|\\hat{\\alpha_{n}} - 0\\big| = \\frac{n+3}{n^3} \\leq \\frac{n+3n}{n^3} = 4 \\cdot \\frac{1}{n^2} = 4\\hat{\\beta_{n}}$$\n",
    "<br>\n",
    "Hence, the rate of convergence of $\\{\\alpha_{n}\\}$ to zero is similar to the convergence of ${1/n}$ to zero, whereas $\\{\\hat{\\alpha_{n}}\\}$ converges to zero at a rate similar to the more rapidly convergent sequence $\\{1/n^2\\}$. We express this by writing\n",
    "<br><br>\n",
    "$$\\alpha_n = 0 + O\\bigg(\\frac{1}{n}\\bigg) \\space\\space and \\space\\space \\hat{\\alpha_n} = 0 + O\\bigg(\\frac{1}{n^2}\\bigg).$$\n",
    "<br>\n",
    "We also use the $O$ (big oh) notation to describe the rate at which functions converge.\n",
    "\n",
    "### Definition 1.19\n",
    "Suppose that $\\lim_{h \\to 0} G(h) = 0$ and $\\lim_{h \\to 0} F(h) = L$. If a positive constant $K$ exists with\n",
    "<br><br>\n",
    "$$\\big|F(h) - L\\big| \\leq K\\big|G(h)\\big|, \\space\\space for \\space sufficiently \\space small \\space h,$$\n",
    "<br>\n",
    "then we write $F(h) = L + O(G(h))$.\n",
    "<br><br>\n",
    "The functions we use for comparison generally have the form $G(h) = h^P$, where $p > 0$. We are interested in the largest value of $p$ for which $F(h) = L + O(h^P)$.\n",
    "\n",
    "### Example 2\n",
    "Use the third Taylor polynomial about $h = 0$ to show that $\\cos h + \\frac{1}{2}h^2 = 1 + O(h^4)$.\n",
    "#### Solution\n",
    "We know that this polynomial is \n",
    "<br><br>\n",
    "$$\\cos h = 1 - \\frac{1}{2}h^2 + \\frac{1}{24}h^4 \\cos\\xi(h),$$\n",
    "<br>\n",
    "for some number \\xi(h) between zero and $h$. This implies that \n",
    "<br><br>\n",
    "$$\\cos h + \\frac{1}{2}h^2 = 1 + \\frac{1}{24}h^4 \\cos\\xi(h).$$\n",
    "<br>\n",
    "Hence, \n",
    "<br><br>\n",
    "$$\\bigg|\\bigg(\\cos h + \\frac{1}{2}h^2\\bigg)\\bigg| - 1 = \\bigg|\\frac{1}{24}\\cos\\xi(h)\\bigg|h^4 \\leq \\frac{1}{24}h^4,$$\n",
    "<br>\n",
    "so as $h \\to 0$, $\\cos h + \\frac{1}{2}h^2$ converges to its limit, 1, about as fast as $h^4$ converges to 0. That is, \n",
    "<br><br>\n",
    "$$\\cos h + \\frac{1}{2}h^2 = 1 + O(h^4).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
